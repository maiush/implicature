{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gws/nopw/j04/ai4er/users/maiush/implicature\n"
     ]
    }
   ],
   "source": [
    "%cd implicature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from constants import results_path\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gemma-2-2b\", \"llama-3.1-8b\", \"mistral-nemo-12b\", \"qwen-2-72b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gemma-2-2b\n",
      "train accuracy: 1.000\n",
      "test accuracy: 0.743\n",
      "test accuracy (instruct): 0.712\n",
      "test accuracy (base): 0.773\n",
      "\n",
      "llama-3.1-8b\n",
      "train accuracy: 1.000\n",
      "test accuracy: 0.789\n",
      "test accuracy (instruct): 0.780\n",
      "test accuracy (base): 0.798\n",
      "\n",
      "mistral-nemo-12b\n",
      "train accuracy: 1.000\n",
      "test accuracy: 0.816\n",
      "test accuracy (instruct): 0.805\n",
      "test accuracy (base): 0.827\n",
      "\n",
      "qwen-2-72b\n",
      "train accuracy: 1.000\n",
      "test accuracy: 0.838\n",
      "test accuracy (instruct): 0.840\n",
      "test accuracy (base): 0.837\n"
     ]
    }
   ],
   "source": [
    "for model_root in models:\n",
    "    print(f\"\\n{model_root}\")\n",
    "    model_instruct = f\"{model_root}-base\"\n",
    "    model_base = f\"{model_root}-instruct\"\n",
    "\n",
    "    # load labels\n",
    "    train_labels = pd.read_json(\"train_data.jsonl\", orient=\"records\", lines=True)[\"implicature\"] == \"yes\"\n",
    "    train_labels = (train_labels * 1).to_numpy()\n",
    "    test_labels = pd.read_json(\"test_data.jsonl\", orient=\"records\", lines=True)[\"implicature\"] == \"yes\"\n",
    "    test_labels = (test_labels * 1).to_numpy()\n",
    "    # double them up - since we train on data from two models\n",
    "    train_len, test_len = len(train_labels), len(test_labels)\n",
    "    train_labels = np.concatenate([train_labels, train_labels])\n",
    "    test_labels = np.concatenate([test_labels, test_labels])\n",
    "\n",
    "    # instantiate classifier\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    # load data\n",
    "    x_train_yes = t.concat([\n",
    "        t.load(f\"{results_path}/{model_name}/harvest_yes_train.pt\", weights_only=True)\n",
    "        for model_name in [model_instruct, model_base]\n",
    "    ], dim=0)\n",
    "    x_train_no = t.concat([\n",
    "        t.load(f\"{results_path}/{model_name}/harvest_no_train.pt\", weights_only=True)\n",
    "        for model_name in [model_instruct, model_base]\n",
    "    ], dim=0)\n",
    "    x_test_yes = t.concat([\n",
    "        t.load(f\"{results_path}/{model_name}/harvest_yes_test.pt\", weights_only=True)\n",
    "        for model_name in [model_instruct, model_base]\n",
    "    ], dim=0)\n",
    "    x_test_no = t.concat([\n",
    "        t.load(f\"{results_path}/{model_name}/harvest_no_test.pt\", weights_only=True)\n",
    "        for model_name in [model_instruct, model_base]\n",
    "    ], dim=0)\n",
    "\n",
    "    # centering data\n",
    "    # yes_mean = x_train_yes.mean(dim=0, keepdim=True)\n",
    "    # no_mean = x_train_no.mean(dim=0, keepdim=True)\n",
    "    # x_train_yes = x_train_yes - yes_mean\n",
    "    # x_train_no = x_train_no - no_mean\n",
    "    # x_test_yes = x_test_yes - yes_mean\n",
    "    # x_test_no = x_test_no - no_mean\n",
    "\n",
    "    # contrast pair differences\n",
    "    x_train = (x_train_yes - x_train_no).float().numpy()\n",
    "    x_test = (x_test_yes - x_test_no).float().numpy()\n",
    "\n",
    "    # fit model\n",
    "    lr.fit(x_train, train_labels)\n",
    "\n",
    "    scores = (lr.score(x_train, train_labels), lr.score(x_test, test_labels))\n",
    "    print(f\"train accuracy: {scores[0]:.3f}\")\n",
    "    print(f\"test accuracy: {scores[1]:.3f}\")\n",
    "    inst_acc = lr.score(x_test[:test_len], test_labels[:test_len])\n",
    "    base_acc = lr.score(x_test[test_len:], test_labels[test_len:])\n",
    "    print(f\"test accuracy (instruct): {inst_acc:.3f}\")\n",
    "    print(f\"test accuracy (base): {base_acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppairs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
